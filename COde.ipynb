{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Automatic Meta-data generation"
      ],
      "metadata": {
        "id": "SSLLLJjFeoZn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eaKx7nYF7qE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9814664-918a-4980-c2cb-60dcc89cf5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  python-dotenv \\\n",
        "  streamlit \\\n",
        "  langdetect \\\n",
        "  langchain-mistralai \\\n",
        "  pytesseract \\\n",
        "  pillow \\\n",
        "  python-docx \\\n",
        "  PyPDF2 \\\n",
        "  pandas"
      ],
      "metadata": {
        "id": "CjrpFeqfdXf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506c993d-893d-40ec-ca57-367b5e4316d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install config -q"
      ],
      "metadata": {
        "id": "2M9zAs6I9FkY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber docx langchain requests python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzGT0EAH_4EO",
        "outputId": "d38db49c-b2ef-4249-cd6b-a1303e447b67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=5b1bb5e5e1b8d5362d543b3e4f4e80d6d7eeb3780d38304adb83096781f29168\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: pypdfium2, docx, pdfminer.six, pdfplumber\n",
            "Successfully installed docx-0.2.4 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#configuration"
      ],
      "metadata": {
        "id": "X4szRaS1ekgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class Config:\n",
        "    # API\n",
        "    MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "    MISTRAL_MODEL = \"mistral-large-latest\"\n",
        "\n",
        "    # File limits\n",
        "    MAX_FILE_SIZE_MB = 300\n",
        "    MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024\n",
        "\n",
        "    # Supported formats\n",
        "    SUPPORTED_EXTENSIONS = ['.pdf', '.docx', '.doc', '.txt', '.xlsx', '.xls', '.md', '.jpg', '.jpeg', '.png', '.tiff', '.bmp']\n",
        "\n",
        "    # Text analysis\n",
        "    DEFAULT_READING_SPEED_WPM = 200\n",
        "    MIN_TEXT_FOR_SUMMARY = 100\n",
        "\n",
        "    # App config\n",
        "    PAGE_TITLE = \"Automatic Meta-Data Generation\"\n",
        "    PAGE_ICON = \"üìÑ\"\n",
        "\n",
        "    @classmethod\n",
        "    def validate(cls):\n",
        "        if not cls.MISTRAL_API_KEY:\n",
        "            raise ValueError(\"MISTRAL_API_KEY not found in .env file\")\n",
        "        return True"
      ],
      "metadata": {
        "id": "vQ454uCKdkBN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Document Loader"
      ],
      "metadata": {
        "id": "iyFAVKU5ezSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from config import Config\n",
        "\n",
        "def validate_file_size(file):\n",
        "    \"\"\"Validate if file size is within limits\"\"\"\n",
        "    if hasattr(file, 'size'):\n",
        "        return file.size <= Config.MAX_FILE_SIZE_BYTES\n",
        "    return True\n",
        "\n",
        "def get_file_type(filename):\n",
        "    \"\"\"Get file type from filename\"\"\"\n",
        "    ext = Path(filename).suffix.lower()\n",
        "    if ext in ['.pdf']:\n",
        "        return 'pdf'\n",
        "    elif ext in ['.docx', '.doc']:\n",
        "        return 'docx'\n",
        "    elif ext in ['.txt']:\n",
        "        return 'txt'\n",
        "    elif ext in ['.xlsx', '.xls']:\n",
        "        return 'excel'\n",
        "    elif ext in ['.md']:\n",
        "        return 'markdown'\n",
        "    elif ext in ['.jpg', '.jpeg', '.png', '.tiff', '.bmp']:\n",
        "        return 'image_ocr'\n",
        "    return 'unknown'\n",
        "\n",
        "def is_supported_format(filename):\n",
        "    \"\"\"Check if file format is supported\"\"\"\n",
        "    ext = Path(filename).suffix.lower()\n",
        "    return ext in Config.SUPPORTED_EXTENSIONS\n",
        "\n",
        "def validate_document(file):\n",
        "    \"\"\"Complete document validation\"\"\"\n",
        "    if not file:\n",
        "        return False, \"No file provided\"\n",
        "\n",
        "    if not is_supported_format(file.name):\n",
        "        return False, f\"Unsupported format. Supported: {', '.join(Config.SUPPORTED_EXTENSIONS)}\"\n",
        "\n",
        "    if not validate_file_size(file):\n",
        "        return False, f\"File too large. Max size: {Config.MAX_FILE_SIZE_MB}MB\"\n",
        "\n",
        "    return True, \"Valid document\"\n",
        "\n",
        "def get_file_info(file):\n",
        "    \"\"\"Get basic file information\"\"\"\n",
        "    return {\n",
        "        'name': file.name,\n",
        "        'size': getattr(file, 'size', 0),\n",
        "        'type': get_file_type(file.name)\n",
        "    }"
      ],
      "metadata": {
        "id": "17hymRYLebw5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detects file type and extracts text from PDF, DOCX, or TXT (with pdfplumber for tables)\n",
        "import os\n",
        "import pdfplumber\n",
        "from docx import Document\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if ext == '.pdf':\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    elif ext == '.docx':\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    elif ext == '.txt':\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type\")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "8rv7ri1eA13A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=extract_text_from_file('/content/Report_Finance.pdf')"
      ],
      "metadata": {
        "id": "7koJFdW1A-I4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleans and normalizes extracted text using regex + NLP techniques\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    # Regex cleaning\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "    # Remove repeating headers/footers\n",
        "    lines = text.splitlines()\n",
        "    line_counts = {}\n",
        "    for line in lines:\n",
        "        line_counts[line] = line_counts.get(line, 0) + 1\n",
        "    lines = [line for line in lines if line_counts[line] < 5]\n",
        "    text = \" \".join(lines)\n",
        "\n",
        "    # NLP preprocessing: stopword removal + lemmatization\n",
        "    doc = nlp(text)\n",
        "    cleaned = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "    return \" \".join(cleaned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUIT8y1xBTIG",
        "outputId": "7fb92cd0-a6b5-4f9f-add8-1dfd0751cf1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_text=preprocess_text(text)"
      ],
      "metadata": {
        "id": "NZr_woBTBxZQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Detection"
      ],
      "metadata": {
        "id": "t2qMF0g0e9s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect, detect_langs\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "def detect_language(text):\n",
        "    \"\"\"Detect the primary language of text\"\"\"\n",
        "    if not text or len(text.strip()) < 10:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    try:\n",
        "        lang_code = detect(text[:1000])  # Use first 1000 chars for efficiency\n",
        "        return get_language_name(lang_code)\n",
        "    except LangDetectException:\n",
        "        return \"Unknown\"\n",
        "\n",
        "def detect_language_with_confidence(text):\n",
        "    \"\"\"Detect language with confidence score\"\"\"\n",
        "    if not text or len(text.strip()) < 10:\n",
        "        return \"Unknown\", 0.0\n",
        "\n",
        "    try:\n",
        "        lang_probs = detect_langs(text[:1000])\n",
        "        if lang_probs:\n",
        "            top_lang = lang_probs[0]\n",
        "            return get_language_name(top_lang.lang), round(top_lang.prob, 2)\n",
        "    except LangDetectException:\n",
        "        pass\n",
        "\n",
        "    return \"Unknown\", 0.0\n",
        "\n",
        "def get_language_name(lang_code):\n",
        "    \"\"\"Convert language code to language name\"\"\"\n",
        "    lang_map = {\n",
        "        'en': 'English',\n",
        "        'es': 'Spanish',\n",
        "        'fr': 'French',\n",
        "        'de': 'German',\n",
        "        'it': 'Italian',\n",
        "        'pt': 'Portuguese',\n",
        "        'ru': 'Russian',\n",
        "        'ja': 'Japanese',\n",
        "        'ko': 'Korean',\n",
        "        'zh-cn': 'Chinese (Simplified)',\n",
        "        'zh-tw': 'Chinese (Traditional)',\n",
        "        'ar': 'Arabic',\n",
        "        'hi': 'Hindi',\n",
        "        'bn': 'Bengali',\n",
        "        'ur': 'Urdu',\n",
        "        'ta': 'Tamil',\n",
        "        'te': 'Telugu',\n",
        "        'mr': 'Marathi',\n",
        "        'gu': 'Gujarati',\n",
        "        'kn': 'Kannada',\n",
        "        'ml': 'Malayalam',\n",
        "        'pa': 'Punjabi',\n",
        "        'ne': 'Nepali',\n",
        "        'si': 'Sinhala',\n",
        "        'th': 'Thai',\n",
        "        'vi': 'Vietnamese',\n",
        "        'id': 'Indonesian',\n",
        "        'ms': 'Malay',\n",
        "        'tl': 'Filipino',\n",
        "        'nl': 'Dutch',\n",
        "        'sv': 'Swedish',\n",
        "        'da': 'Danish',\n",
        "        'no': 'Norwegian',\n",
        "        'fi': 'Finnish',\n",
        "        'pl': 'Polish',\n",
        "        'cs': 'Czech',\n",
        "        'sk': 'Slovak',\n",
        "        'hu': 'Hungarian',\n",
        "        'ro': 'Romanian',\n",
        "        'bg': 'Bulgarian',\n",
        "        'hr': 'Croatian',\n",
        "        'sr': 'Serbian',\n",
        "        'sl': 'Slovenian',\n",
        "        'et': 'Estonian',\n",
        "        'lv': 'Latvian',\n",
        "        'lt': 'Lithuanian',\n",
        "        'tr': 'Turkish',\n",
        "        'el': 'Greek',\n",
        "        'he': 'Hebrew',\n",
        "        'fa': 'Persian',\n",
        "        'sw': 'Swahili',\n",
        "        'af': 'Afrikaans'\n",
        "    }\n",
        "\n",
        "    return lang_map.get(lang_code, f\"Unknown ({lang_code})\")\n",
        "\n",
        "def analyze_language(text):\n",
        "    \"\"\"Complete language analysis\"\"\"\n",
        "    language, confidence = detect_language_with_confidence(text)\n",
        "\n",
        "    return {\n",
        "        'detected_language': language,\n",
        "        'confidence': f\"{confidence * 100:.1f}%\",\n",
        "        'is_reliable': confidence > 0.7\n",
        "    }"
      ],
      "metadata": {
        "id": "H9yi2DGOeieq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=50):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_text(text)"
      ],
      "metadata": {
        "id": "BMVB_q7zB3vE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#File Handling"
      ],
      "metadata": {
        "id": "GjwhSVnxe5Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "from config import Config\n",
        "\n",
        "def handle_file_upload():\n",
        "    \"\"\"Handle file upload in Streamlit\"\"\"\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Upload your document\",\n",
        "        type=[ext.replace('.', '') for ext in Config.SUPPORTED_EXTENSIONS],\n",
        "        help=f\"Supported formats: {', '.join(Config.SUPPORTED_EXTENSIONS)} | Max size: {Config.MAX_FILE_SIZE_MB}MB\"\n",
        "    )\n",
        "\n",
        "    return uploaded_file\n",
        "\n",
        "def save_temporary_file(uploaded_file):\n",
        "    \"\"\"Save uploaded file to temporary location\"\"\"\n",
        "    if not uploaded_file:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Create temp directory if it doesn't exist\n",
        "        temp_dir = Path(\"temp_files\")\n",
        "        temp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Create temporary file\n",
        "        file_extension = Path(uploaded_file.name).suffix\n",
        "        temp_file = tempfile.NamedTemporaryFile(\n",
        "            delete=False,\n",
        "            suffix=file_extension,\n",
        "            dir=temp_dir\n",
        "        )\n",
        "\n",
        "        # Write file content\n",
        "        temp_file.write(uploaded_file.read())\n",
        "        temp_file.close()\n",
        "\n",
        "        # Reset file pointer for further use\n",
        "        uploaded_file.seek(0)\n",
        "\n",
        "        return temp_file.name\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error saving file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def cleanup_temp_files():\n",
        "    \"\"\"Clean up temporary files\"\"\"\n",
        "    try:\n",
        "        temp_dir = Path(\"temp_files\")\n",
        "        if temp_dir.exists():\n",
        "            for file_path in temp_dir.glob(\"*\"):\n",
        "                if file_path.is_file():\n",
        "                    file_path.unlink()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not cleanup temp files: {e}\")\n",
        "\n",
        "def get_file_size_mb(file):\n",
        "    \"\"\"Get file size in MB\"\"\"\n",
        "    if hasattr(file, 'size'):\n",
        "        return round(file.size / (1024 * 1024), 2)\n",
        "    return 0\n",
        "\n",
        "def display_file_info(file):\n",
        "    \"\"\"Display file information in Streamlit\"\"\"\n",
        "    if file:\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            st.metric(\"File Name\", file.name)\n",
        "\n",
        "        with col2:\n",
        "            file_size = get_file_size_mb(file)\n",
        "            st.metric(\"File Size\", f\"{file_size} MB\")\n",
        "\n",
        "        with col3:\n",
        "            file_type = Path(file.name).suffix.upper()\n",
        "            st.metric(\"File Type\", file_type)\n",
        "\n",
        "def validate_uploaded_file(file):\n",
        "    \"\"\"Validate uploaded file\"\"\"\n",
        "    if not file:\n",
        "        return False, \"No file uploaded\"\n",
        "\n",
        "    # Check file size\n",
        "    if get_file_size_mb(file) > Config.MAX_FILE_SIZE_MB:\n",
        "        return False, f\"File size exceeds {Config.MAX_FILE_SIZE_MB}MB limit\"\n",
        "\n",
        "    # Check file extension\n",
        "    file_ext = Path(file.name).suffix.lower()\n",
        "    if file_ext not in Config.SUPPORTED_EXTENSIONS:\n",
        "        return False, f\"Unsupported file format. Supported: {', '.join(Config.SUPPORTED_EXTENSIONS)}\"\n",
        "\n",
        "    return True, \"File is valid\""
      ],
      "metadata": {
        "id": "ofh8VthZeftK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Meta-Data Generation"
      ],
      "metadata": {
        "id": "7YTbYM69fbFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def get_extraction_timestamp():\n",
        "    \"\"\"Get current timestamp for extraction\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def calculate_document_length(text):\n",
        "    \"\"\"Calculate document length in characters\"\"\"\n",
        "    return len(text) if text else 0\n",
        "\n",
        "def count_words(text):\n",
        "    \"\"\"Count words in text\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(text.split())\n",
        "\n",
        "def count_paragraphs(text):\n",
        "    \"\"\"Count paragraphs in text\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    return len(paragraphs)\n",
        "\n",
        "def calculate_reading_time(word_count, wpm=200):\n",
        "    \"\"\"Calculate approximate reading time in minutes\"\"\"\n",
        "    if word_count <= 0:\n",
        "        return \"0 min\"\n",
        "\n",
        "    minutes = word_count / wpm\n",
        "\n",
        "    if minutes < 1:\n",
        "        return f\"{int(minutes * 60)} sec\"\n",
        "    elif minutes < 60:\n",
        "        return f\"{int(minutes)} min\"\n",
        "    else:\n",
        "        hours = int(minutes // 60)\n",
        "        mins = int(minutes % 60)\n",
        "        return f\"{hours}h {mins}min\"\n",
        "\n",
        "def format_file_size(size_bytes):\n",
        "    \"\"\"Format file size in human readable format\"\"\"\n",
        "    if size_bytes == 0:\n",
        "        return \"0 B\"\n",
        "\n",
        "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
        "        if size_bytes < 1024.0:\n",
        "            return f\"{size_bytes:.1f} {unit}\"\n",
        "        size_bytes /= 1024.0\n",
        "    return f\"{size_bytes:.1f} TB\"\n",
        "\n",
        "def generate_basic_metadata(file, text, file_type):\n",
        "    \"\"\"Generate comprehensive metadata for document\"\"\"\n",
        "    file_size = getattr(file, 'size', 0)\n",
        "    word_count = count_words(text) # Calculate word count\n",
        "\n",
        "    metadata = {\n",
        "        'file_name': file.name,\n",
        "        'extracted_on': get_extraction_timestamp(),\n",
        "        'file_type': file_type.upper(),\n",
        "        'file_size': format_file_size(file_size),\n",
        "        'document_length': f\"{calculate_document_length(text):,} characters\",\n",
        "        'word_count': f\"{word_count:,} words\", # Use calculated word count\n",
        "        'approx_reading_time': calculate_reading_time(word_count), # Pass word count\n",
        "        'paragraphs': f\"{count_paragraphs(text)} paragraphs\"\n",
        "    }\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def clean_text_for_analysis(text):\n",
        "    \"\"\"Clean text for better analysis\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove extra whitespace and normalize\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "MADo4Fo7fUAl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an intelligent assistant designed to understand documents and extract structured information from them.\n",
        "\n",
        "Your task is to:\n",
        "1. Extract the following metadata:\n",
        "   - Title (if mentioned)\n",
        "   - Author (if available)\n",
        "   - Date of publication or document creation (if available)\n",
        "   - Keywords or topics covered\n",
        "   - Type of document (choose from: research paper, legal notice, resume, report, book chapter, article, business proposal, letter, others)\n",
        "2. Generate a concise summary of the content (3-5 sentences).\n",
        "\n",
        "Read the content below and return your answer in this JSON format:\n",
        "{{\n",
        "  \"title\": \"\",\n",
        "  \"author\": \"\",\n",
        "  \"date\": \"\",\n",
        "  \"keywords\": [],\n",
        "  \"document_type\": \"\",\n",
        "  \"summary\": \"\"\n",
        "}}\n",
        "\n",
        "Content:\n",
        "\\\"\\\"\\\"{content_chunk}\\\"\\\"\\\"\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "3cX4r70wCLXH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. llm_call.py\n",
        "import os\n",
        "import requests\n",
        "\n",
        "os.environ[\"MISTRAL_API_URL\"] = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"vzvzkE3QKin6nqTVUTwpyOwRQT2xLdkf\"\n"
      ],
      "metadata": {
        "id": "0RjOcqvlCPvB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MISTRAL_API_URL = os.getenv(\"MISTRAL_API_URL\")\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "def call_llm_on_chunk(chunk):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(content_chunk=chunk)}\n",
        "        ],\n",
        "        \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"‚ùå Error {response.status_code}: {response.text}\")\n",
        "        return \"ERROR\"\n",
        "    return response.json()['choices'][0]['message']['content']\n"
      ],
      "metadata": {
        "id": "lM2BvORhCagt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary generation"
      ],
      "metadata": {
        "id": "W5YFstukfio3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "from config import Config\n",
        "\n",
        "def initialize_mistral():\n",
        "    \"\"\"Initialize Mistral AI client\"\"\"\n",
        "    try:\n",
        "        return ChatMistralAI(\n",
        "            mistral_api_key=Config.MISTRAL_API_KEY,\n",
        "            model=Config.MISTRAL_MODEL,\n",
        "            temperature=0.3\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Mistral: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_summary(text, max_words=200):\n",
        "    \"\"\"Generate document summary using Mistral AI\"\"\"\n",
        "    if not text or len(text.strip()) < Config.MIN_TEXT_FOR_SUMMARY:\n",
        "        return \"Text too short for summary generation\"\n",
        "\n",
        "    mistral = initialize_mistral()\n",
        "    if not mistral:\n",
        "        return \"Error: Could not initialize Mistral AI\"\n",
        "\n",
        "    # Truncate text if too long (keep first 3000 chars for efficiency)\n",
        "    text_sample = text[:3000] if len(text) > 3000 else text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Please provide a concise summary of the following document in approximately {max_words} words.\n",
        "    Focus on the main points, key findings, and important information.\n",
        "\n",
        "    Document text:\n",
        "    {text_sample}\n",
        "\n",
        "    Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = mistral.invoke(prompt)\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating summary: {str(e)}\"\n",
        "\n",
        "def extract_key_points(text, num_points=5):\n",
        "    \"\"\"Extract key points from document\"\"\"\n",
        "    if not text or len(text.strip()) < Config.MIN_TEXT_FOR_SUMMARY:\n",
        "        return []\n",
        "\n",
        "    mistral = initialize_mistral()\n",
        "    if not mistral:\n",
        "        return [\"Error: Could not initialize Mistral AI\"]\n",
        "\n",
        "    text_sample = text[:3000] if len(text) > 3000 else text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Extract the {num_points} most important key points from this document.\n",
        "    Present them as a numbered list, each point should be concise and informative.\n",
        "\n",
        "    Document text:\n",
        "    {text_sample}\n",
        "\n",
        "    Key Points:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = mistral.invoke(prompt)\n",
        "        points = response.content.strip().split('\\n')\n",
        "        return [point.strip() for point in points if point.strip()][:num_points]\n",
        "    except Exception as e:\n",
        "        return [f\"Error extracting key points: {str(e)}\"]\n",
        "\n",
        "def generate_document_insights(text):\n",
        "    \"\"\"Generate comprehensive document insights\"\"\"\n",
        "    if not text or len(text.strip()) < Config.MIN_TEXT_FOR_SUMMARY:\n",
        "        return {\n",
        "            'summary': \"Text too short for analysis\",\n",
        "            'key_points': [],\n",
        "            'document_type': \"Unknown\"\n",
        "        }\n",
        "\n",
        "    summary = generate_summary(text)\n",
        "    key_points = extract_key_points(text)\n",
        "    doc_type = classify_document_type(text)\n",
        "\n",
        "    return {\n",
        "        'summary': summary,\n",
        "        'key_points': key_points,\n",
        "        'document_type': doc_type\n",
        "    }\n",
        "\n",
        "def classify_document_type(text):\n",
        "    \"\"\"Classify document type based on content\"\"\"\n",
        "    if not text:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    mistral = initialize_mistral()\n",
        "    if not mistral:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    text_sample = text[:1000]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Classify this document type in one or two words (e.g., Report, Research Paper, Manual, Letter, Article, etc.):\n",
        "\n",
        "    {text_sample}\n",
        "\n",
        "    Document Type:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = mistral.invoke(prompt)\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        return \"Unknown\""
      ],
      "metadata": {
        "id": "RKoUGsGwfhmI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_document_chunks(chunks):\n",
        "    results = []\n",
        "    for chunk in chunks:\n",
        "        result = call_llm_on_chunk(chunk)\n",
        "        results.append(result)\n",
        "    return results"
      ],
      "metadata": {
        "id": "Ea7jUa-dCgS3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Analyzer"
      ],
      "metadata": {
        "id": "P2PDRlngftkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def count_words(text):\n",
        "    \"\"\"Count total words in text\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(text.split())\n",
        "\n",
        "def count_sentences(text):\n",
        "    \"\"\"Count sentences in text\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    return len([s for s in sentences if s.strip()])\n",
        "\n",
        "def count_paragraphs(text):\n",
        "    \"\"\"Count paragraphs in text\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    return len(paragraphs)\n",
        "\n",
        "def count_lines(text):\n",
        "    \"\"\"Count lines in text\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len([line for line in text.split('\\n') if line.strip()])\n",
        "\n",
        "def analyze_readability(text):\n",
        "    \"\"\"Basic readability analysis\"\"\"\n",
        "    if not text:\n",
        "        return \"N/A\"\n",
        "\n",
        "    words = count_words(text)\n",
        "    sentences = count_sentences(text)\n",
        "\n",
        "    if sentences == 0:\n",
        "        return \"N/A\"\n",
        "\n",
        "    avg_words_per_sentence = words / sentences\n",
        "\n",
        "    if avg_words_per_sentence < 15:\n",
        "        return \"Easy\"\n",
        "    elif avg_words_per_sentence < 20:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Hard\"\n",
        "\n",
        "def get_most_common_words(text, top_n=10):\n",
        "    \"\"\"Get most common words (excluding common stop words)\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Simple stop words\n",
        "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "                  'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have',\n",
        "                  'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',\n",
        "                  'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'}\n",
        "\n",
        "    # Clean and split text\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
        "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    return Counter(filtered_words).most_common(top_n)\n",
        "\n",
        "def calculate_text_complexity(text):\n",
        "    \"\"\"Calculate text complexity metrics\"\"\"\n",
        "    if not text:\n",
        "        return {}\n",
        "\n",
        "    words = count_words(text)\n",
        "    sentences = count_sentences(text)\n",
        "    characters = len(text.replace(' ', ''))\n",
        "\n",
        "    return {\n",
        "        'avg_word_length': round(characters / words, 1) if words > 0 else 0,\n",
        "        'avg_sentence_length': round(words / sentences, 1) if sentences > 0 else 0,\n",
        "        'readability': analyze_readability(text)\n",
        "    }\n",
        "\n",
        "def analyze_text_structure(text):\n",
        "    \"\"\"Complete text structure analysis\"\"\"\n",
        "    if not text:\n",
        "        return {}\n",
        "\n",
        "    analysis = {\n",
        "        'word_count': count_words(text),\n",
        "        'sentence_count': count_sentences(text),\n",
        "        'paragraph_count': count_paragraphs(text),\n",
        "        'line_count': count_lines(text),\n",
        "        'character_count': len(text),\n",
        "        'character_count_no_spaces': len(text.replace(' ', '')),\n",
        "        'complexity': calculate_text_complexity(text),\n",
        "        'top_words': get_most_common_words(text, 5)\n",
        "    }\n",
        "\n",
        "    return analysis"
      ],
      "metadata": {
        "id": "5cc_dbvjfpx-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text-Extractor"
      ],
      "metadata": {
        "id": "k29x6jwUf1ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "def extract_from_pdf(file):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting PDF: {str(e)}\"\n",
        "\n",
        "def extract_from_docx(file):\n",
        "    \"\"\"Extract text from DOCX file\"\"\"\n",
        "    try:\n",
        "        doc = Document(file)\n",
        "        text = \"\"\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + \"\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting DOCX: {str(e)}\"\n",
        "\n",
        "def extract_from_txt(file):\n",
        "    \"\"\"Extract text from TXT file\"\"\"\n",
        "    try:\n",
        "        content = file.read()\n",
        "        if isinstance(content, bytes):\n",
        "            content = content.decode('utf-8')\n",
        "        return content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting TXT: {str(e)}\"\n",
        "\n",
        "def extract_from_excel(file):\n",
        "    \"\"\"Extract text from Excel file\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(file, sheet_name=None)\n",
        "        text = \"\"\n",
        "        for sheet_name, sheet_df in df.items():\n",
        "            text += f\"Sheet: {sheet_name}\\n\"\n",
        "            text += sheet_df.to_string(index=False) + \"\\n\\n\"\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting Excel: {str(e)}\"\n",
        "\n",
        "def extract_from_image_ocr(file):\n",
        "    \"\"\"Extract text from image using OCR\"\"\"\n",
        "    try:\n",
        "        image = Image.open(file)\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting OCR: {str(e)}\"\n",
        "\n",
        "def extract_from_markdown(file):\n",
        "    \"\"\"Extract text from Markdown file\"\"\"\n",
        "    try:\n",
        "        content = file.read()\n",
        "        if isinstance(content, bytes):\n",
        "            content = content.decode('utf-8')\n",
        "        return content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting Markdown: {str(e)}\"\n",
        "\n",
        "def extract_text(file, file_type):\n",
        "    \"\"\"Main text extraction function\"\"\"\n",
        "    extractors = {\n",
        "        'pdf': extract_from_pdf,\n",
        "        'docx': extract_from_docx,\n",
        "        'txt': extract_from_txt,\n",
        "        'excel': extract_from_excel,\n",
        "        'image_ocr': extract_from_image_ocr,\n",
        "        'markdown': extract_from_markdown\n",
        "    }\n",
        "\n",
        "    extractor = extractors.get(file_type)\n",
        "    if not extractor:\n",
        "        return f\"Unsupported file type: {file_type}\"\n",
        "\n",
        "    # Reset file pointer\n",
        "    if hasattr(file, 'seek'):\n",
        "        file.seek(0)\n",
        "\n",
        "    return extractor(file)"
      ],
      "metadata": {
        "id": "5zNHUGDvfyuV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_file('/content/Report_Finance.pdf')\n",
        "clean_text = preprocess_text(text)\n",
        "chunks = split_text_into_chunks(clean_text)"
      ],
      "metadata": {
        "id": "sPpU_RvwClOn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert sentence-transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67MCYOJnCt1m",
        "outputId": "b9bf2945-27c5-48c2-cb31-07a165cbeb44"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"‚úÖ Total Chunks: {len(chunks)}\")\n",
        "\n",
        "results = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"\\n--- Generating summary for Chunk {i+1}/{len(chunks)} ---\")\n",
        "    summary = call_llm_on_chunk(chunk)\n",
        "    print(summary)\n",
        "    results.append(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDmMw__bDOZP",
        "outputId": "e17c72e4-2dfb-4207-8f0b-aa2aa9213655"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Total Chunks: 12\n",
            "\n",
            "--- Generating summary for Chunk 1/12 ---\n",
            "{\n",
            "  \"title\": \"Credit Card Behaviour Score prediction Classification Risk base Techniques\",\n",
            "  \"author\": \"Mahanti Ajay Babu\",\n",
            "  \"date\": \"\",\n",
            "  \"keywords\": [\"Credit Card\", \"Behaviour Score\", \"Classification\", \"Risk\", \"Techniques\", \"Summer Project\", \"EDA\", \"Data Preprocess\", \"Drop Repeated Categories\", \"education\", \"marriage\", \"Age\", \"bill statement\", \"previous\"],\n",
            "  \"document_type\": \"report\",\n",
            "  \"summary\": \"This report, titled 'Credit Card Behaviour Score prediction Classification Risk base Techniques', is a summer project submission by Mahanti Ajay Babu. The project involves the creation of a risk model for credit cards, data preprocessing, and drop of repeated categories. The report also includes an education feature, marriage feature, data visualization, and target variable for sex and age. The project was guided by the Finance Club of the Department of Mechanical Engineering at the Indian Institute of Technology Roorkee.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 2/12 ---\n",
            "{\n",
            "  \"title\": \"Age Variable Variable bill statement correlation analysis using SMOTE Algorithm and ML model\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"Credit risk\", \"Machine Learning\", \"SMOTE Algorithm\", \"Correlation Analysis\", \"Predict default\", \"Credit card\", \"Data preprocessing\", \"ROC Curve\", \"Confusion Matrices\"],\n",
            "  \"document_type\": \"Research paper\",\n",
            "  \"summary\": \"This document discusses the use of Machine Learning (ML) models to analyze credit risk, specifically focusing on predicting potential debtors and managing credit risk. The paper mentions the use of the SMOTE Algorithm for data preprocessing, and the importance of droping repeated categories. The goal is to improve the accuracy of credit risk analysis and provide a scientific method for lenders to identify potential debtors.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 3/12 ---\n",
            "{\n",
            "  \"title\": \"Not specified\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"Pre processing\", \"Drop Repeated Categories\", \"categorical variable\", \"Redundancy\", \"Model Confusion\", \"Loss Model\", \"Interpretability\", \"Education\", \"Exploratory Data Analysis\", \"EDUCATION feature\", \"Graduate School\", \"University\", \"High School\", \"Feature\", \"Redundancy\", \"Code\", \"Variable\", \"Graduate School University High School\", \"Multiple feature\", \"Weakly define category\", \"New EDUCATION feature\", \"Meaningful\", \"Valid\", \"Summary\", \"Account\", \"Education profile\", \"Credit cardholder\", \"Overall analysis\", \"Modeling flow\", \"Well\", \"Complication\", \"Decrease potential overfitting\", \"Perform\", \"Small sub class\", \"Marriage\"],\n",
            "  \"document_type\": \"Not specified\",\n",
            "  \"summary\": \"The document discusses the concept of pre-processing categorical variables, redundancy, and its impact on model performance. It also touches upon the importance of Exploratory Data Analysis (EDA) and the handling of education-related features such as Graduate School, University, and High School. The document suggests methods to simplify variable collapse and decrease potential overfitting, and mentions the importance of overall analysis and modeling flow.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 4/12 ---\n",
            "{\n",
            "  \"title\": \"No explicit title provided\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not available\",\n",
            "  \"keywords\": [\"overfitting\", \"small sub class\", \"marriage feature\", \"exploratory datum analysis\", \"data analysis\", \"variable transformation\", \"model interpretability\", \"visualization\", \"balanced imbalance data\", \"customer failure\"],\n",
            "  \"document_type\": \"Article\",\n",
            "  \"summary\": \"The document discusses the impact of overfitting on a small subclass, specifically the 'marriage feature', in the context of data analysis. It suggests methods to reduce redundancy, improve semantic coherence, and maintain a reliable feature set to prevent overfitting. The author also emphasizes the importance of visualization for understanding balanced and imbalanced data, and the use of clear visual representations to quickly observe class distributions.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 5/12 ---\n",
            "{\n",
            "  \"title\": \"Guide to Handling Class Imbalance in Machine Learning\",\n",
            "  \"author\": \"Not specified in the provided content\",\n",
            "  \"date\": \"Not specified in the provided content\",\n",
            "  \"keywords\": [\"Machine Learning\", \"Class Imbalance\", \"Evaluation Metric\", \"Model\", \"Data Distribution\", \"Gender\", \"Exploratory Data Analysis\"],\n",
            "  \"document_type\": \"Article\",\n",
            "  \"summary\": \"The article discusses a guide for handling class imbalance in machine learning, emphasizing the importance of choosing appropriate techniques and suitable evaluation metrics. It also highlights the role of gender in customer base analysis and the need for exploratory data analysis to understand the distribution of sex variables. Additionally, the article touches upon the importance of simplifying education variable representation and combining single categories to avoid redundancy.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 6/12 ---\n",
            "{\n",
            "  \"title\": \"Exploratory Data Analysis on Marriage Status and Age in a Credit Card Holder Dataset\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"Exploratory Data Analysis\", \"Marriage Status\", \"Age\", \"Credit Card Holder Dataset\", \"Distribution\", \"Fraud\"],\n",
            "  \"document_type\": \"Research Paper\",\n",
            "  \"summary\": \"This research paper focuses on an exploratory data analysis of a credit card holder dataset, examining the distribution of marriage status and age. The goal is to reduce redundancy, simplify analysis, and make the distribution balanced to help the model learn effectively. The study also aims to understand the key demographic characteristics and potential fraud patterns related to marital status and age.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 7/12 ---\n",
            "{\n",
            "  \"title\": \"Analyzing the Influence of Age on Customer Financial Behavior and Risk Profile\",\n",
            "  \"author\": \"\",\n",
            "  \"date\": \"\",\n",
            "  \"keywords\": [\"age\", \"customer\", \"financial behavior\", \"risk profile\", \"credit risk model\", \"credit limit\", \"credit fraud\", \"default\", \"distribution\", \"outlier\", \"cluster\", \"portfolio\", \"credit stability\"],\n",
            "  \"document_type\": \"research paper\",\n",
            "  \"summary\": \"This document discusses the impact of age on customer financial behavior and risk profile, with a focus on developing a robust and accurate credit risk model. The study suggests that older customers are more likely to use credit cards and may be prone to credit fraud. The analysis of credit limits and credit history is highlighted as an important step in understanding customer financial profiles, and the presence of outliers or unusual age ranges is observed to help uncover patterns. The document also mentions the use of a default histogram and the importance of examining the distribution range to help predict future defaults.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 8/12 ---\n",
            "{\n",
            "  \"title\": \"Exploratory Data Analysis for Credit Risk Prediction\",\n",
            "  \"author\": \"\",\n",
            "  \"date\": \"\",\n",
            "  \"keywords\": [\"Credit Risk Prediction\", \"Exploratory Data Analysis\", \"Bill Statement\", \"Payment History\", \"Default Risk\", \"Correlation Analysis\"],\n",
            "  \"document_type\": \"Research Paper\",\n",
            "  \"summary\": \"This research paper aims to investigate and analyze customer financial habits by exploring patterns in bill statements and payment histories to help predict future payment defaults. It discusses the importance of key indicators such as the total bill statement, payment variables, and the amount paid subsequent months. The paper also mentions the use of Correlation Analysis to explore relationships between variables and the outcome of default. The analysis guides the choice of variables to focus on, and provides valuable insights into drivers of credit risk.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 9/12 ---\n",
            "{\n",
            "  \"title\": \"SMOTE Algorithm Application in Credit Risk Portfolio\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"SMOTE Algorithm\", \"Credit Risk Portfolio\", \"Data Synthesis\", \"Classifier Training\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\", \"Optuna\", \"Credit Card\"],\n",
            "  \"document_type\": \"Research Paper\",\n",
            "  \"summary\": \"This document discusses the application of the SMOTE algorithm in credit risk portfolio management. The SMOTE algorithm is used to handle imbalanced data and predict customer default. The paper trains different classifiers like Logistic Regression, Decision Tree, Random Forest, and XGBoost using the SMOTE algorithm. It also mentions the use of Optuna for hyperparameter optimization to fine-tune the performance of the XGBoost classifier.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 10/12 ---\n",
            "{\n",
            "  \"title\": \"Tune Performance XGBoost Classifier for Credit Card Default Prediction\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"XGBoost\", \"Classifier\", \"Credit Card Default Prediction\", \"Performance Optimization\", \"Recall\", \"Precision\", \"Financial Risk\", \"Optuna\", \"Hyperparameter Optimization\", \"Confusion Matrix\"],\n",
            "  \"document_type\": \"Article\",\n",
            "  \"summary\": \"This document discusses the process of tuning an XGBoost classifier for credit card default prediction, focusing on maximizing the score while emphasizing recall and precision. The importance of financial risk perspective is highlighted, as incorrectly identifying a defaulter can be costly. The search space for key hyperparameters such as eta, subsample, lambda, alpha, gamma is defined, and the use of Optuna for systematic trials is mentioned. The document also touches upon the trial and error approach, the process of improving the baseline, and the importance of hyperparameter optimization in developing a robust and reliable classifier to help financial institutions manage risk effectively.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 11/12 ---\n",
            "{\n",
            "  \"title\": \"Machine Learning model predicts credit card defaulters\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"XGBoost classifier\", \"confusion matrix\", \"true positive\", \"true negative\", \"false positive\", \"false negative\", \"ROC Curve\", \"AUC\", \"credit card\", \"credit card defaulters\"],\n",
            "  \"document_type\": \"article\",\n",
            "  \"summary\": \"This article discusses the use of XGBoost classifier to predict credit card defaulters. The classifier's ability to distinguish defaulters from non-defaulters is summarized through the confusion matrix. The importance of a high true positive rate and a low false negative rate is emphasized to minimize financial risk. The ROC Curve and AUC are mentioned as graphical views of the classifier's ability to separate classes, with a strong AUC indicating strong discriminative power. The article concludes that a tuned XGBoost model can effectively distinguish defaulter and non-defaulter cases with different scoring thresholds.\"\n",
            "}\n",
            "\n",
            "--- Generating summary for Chunk 12/12 ---\n",
            "{\n",
            "  \"title\": \"Machine Learning model predicts credit card default\",\n",
            "  \"author\": \"Not specified\",\n",
            "  \"date\": \"Not specified\",\n",
            "  \"keywords\": [\"Machine Learning\", \"Credit Card Default\", \"XGBoost\", \"Logistic Regression\", \"Decision Tree\", \"Risk Management\", \"Optuna\", \"Hyperparameter Tuning\", \"Model Evaluation\"],\n",
            "  \"document_type\": \"Article\",\n",
            "  \"summary\": \"This article discusses the use of machine learning models, specifically XGBoost and Logistic Regression, to predict credit card defaults. It highlights the superior performance of XGBoost in identifying defaulters and minimizing false negatives, emphasizing its importance for effective risk management in financial institutions. The article also mentions the importance of tuning hyperparameters using Optuna to improve model scores. The overall approach is presented as a proactive method for identifying high-risk borrowers, potentially reducing financial losses. References are made to online courses, blogs, and official documentation for further learning.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utilisation"
      ],
      "metadata": {
        "id": "96znQG1Rf9xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "def format_file_size(size_bytes):\n",
        "    \"\"\"Format file size in human readable format\"\"\"\n",
        "    if size_bytes == 0:\n",
        "        return \"0 B\"\n",
        "\n",
        "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
        "        if size_bytes < 1024.0:\n",
        "            return f\"{size_bytes:.1f} {unit}\"\n",
        "        size_bytes /= 1024.0\n",
        "    return f\"{size_bytes:.1f} TB\"\n",
        "\n",
        "def format_timestamp(timestamp=None):\n",
        "    \"\"\"Format timestamp for display\"\"\"\n",
        "    if timestamp is None:\n",
        "        timestamp = datetime.now()\n",
        "\n",
        "    if isinstance(timestamp, str):\n",
        "        return timestamp\n",
        "\n",
        "    return timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove special characters that might cause issues\n",
        "    text = re.sub(r'[^\\w\\s\\.,!?;:\\-\\(\\)\\\"\\']+', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def truncate_text(text, max_length=100):\n",
        "    \"\"\"Truncate text with ellipsis\"\"\"\n",
        "    if not text or len(text) <= max_length:\n",
        "        return text\n",
        "\n",
        "    return text[:max_length].rsplit(' ', 1)[0] + \"...\"\n",
        "\n",
        "def format_number(number):\n",
        "    \"\"\"Format numbers with commas\"\"\"\n",
        "    if isinstance(number, (int, float)):\n",
        "        return f\"{number:,}\"\n",
        "    return str(number)\n",
        "\n",
        "def calculate_reading_time(word_count, wpm=200):\n",
        "    \"\"\"Calculate reading time from word count\"\"\"\n",
        "    if word_count <= 0:\n",
        "        return \"0 min\"\n",
        "\n",
        "    minutes = word_count / wpm\n",
        "\n",
        "    if minutes < 1:\n",
        "        return f\"{int(minutes * 60)} sec\"\n",
        "    elif minutes < 60:\n",
        "        return f\"{int(minutes)} min\"\n",
        "    else:\n",
        "        hours = int(minutes // 60)\n",
        "        mins = int(minutes % 60)\n",
        "        return f\"{hours}h {mins}min\"\n",
        "\n",
        "def safe_divide(numerator, denominator):\n",
        "    \"\"\"Safe division with zero check\"\"\"\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    return numerator / denominator\n",
        "\n",
        "def export_metadata_json(metadata):\n",
        "    \"\"\"Export metadata as JSON string\"\"\"\n",
        "    try:\n",
        "        return json.dumps(metadata, indent=2, ensure_ascii=False)\n",
        "    except Exception as e:\n",
        "        return f\"Error exporting JSON: {str(e)}\"\n",
        "\n",
        "def get_file_extension(filename):\n",
        "    \"\"\"Get file extension from filename\"\"\"\n",
        "    return Path(filename).suffix.lower()\n",
        "\n",
        "def is_text_meaningful(text, min_length=10):\n",
        "    \"\"\"Check if text has meaningful content\"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "\n",
        "    # Remove whitespace and check length\n",
        "    clean = text.strip()\n",
        "    if len(clean) < min_length:\n",
        "        return False\n",
        "\n",
        "    # Check if text has alphabetic characters\n",
        "    if not re.search(r'[a-zA-Z]', clean):\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "znfxn2HLf6Ze"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running with example"
      ],
      "metadata": {
        "id": "DFjgZHa0hR45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After summarizing all chunks:\n",
        "combined_summaries = \"\\n\\n\".join(results)\n",
        "\n",
        "combine_prompt = f\"\"\"\n",
        "You are a smart assistant. Below are multiple partial summaries of a document, generated from different parts.\n",
        "\n",
        "Your task is to combine them into a **single metadata + summary JSON**, like this:\n",
        "{{\n",
        "  \"title\": \"\",\n",
        "  \"author\": \"\",\n",
        "  \"date\": \"\",\n",
        "  \"keywords\": [],\n",
        "  \"document_type\": \"\",\n",
        "  \"summary\": \"\"\n",
        "}}\n",
        "\n",
        "Summaries:\n",
        "\\\"\\\"\\\"{combined_summaries}\\\"\\\"\\\"\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "dtmxwKdL-K30"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "import re\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def call_llm_merge_summary(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"open-mistral-7b\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
        "    return response.json()['choices'][0]['message']['content']\n",
        "\n",
        "final_output = call_llm_merge_summary(combine_prompt)"
      ],
      "metadata": {
        "id": "jX-jTvWoDY9z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# üéØ Final Output Handling\n",
        "# ============================\n",
        "try:\n",
        "    # Use regex to extract the JSON part from the string\n",
        "    json_match = re.search(r'```json\\n(.*?)\\n```', final_output, re.DOTALL)\n",
        "    if json_match:\n",
        "        json_string = json_match.group(1)\n",
        "        parsed = json.loads(json_string)\n",
        "\n",
        "        # ‚úÖ Improve keywords using KeyBERT\n",
        "        # Check if clean_text is available before using KeyBERT\n",
        "        if 'text' in locals(): # Use the original 'text' variable\n",
        "            kw_model = KeyBERT(model=SentenceTransformer('all-MiniLM-L6-v2'))\n",
        "            # Pass the original 'text' as a list containing one element\n",
        "            kb_keywords = kw_model.extract_keywords(\n",
        "                [text],\n",
        "                keyphrase_ngram_range=(1, 2),\n",
        "                stop_words='english',\n",
        "                top_n=10,\n",
        "                use_maxsum=True,\n",
        "                nr_candidates=20\n",
        "            )\n",
        "            final_keywords = [kw for kw, score in kb_keywords]\n",
        "            parsed[\"keywords\"] = final_keywords\n",
        "        else:\n",
        "            print(\"Warning: 'text' not available for keyword extraction using KeyBERT.\")\n",
        "\n",
        "\n",
        "        # ‚úÖ Pretty output\n",
        "        print(\"\\n‚úÖ Final Metadata:\")\n",
        "        print(json.dumps(parsed, indent=2))\n",
        "\n",
        "        print(\"\\n‚úÖ Final Summary:\")\n",
        "        if \"summary\" in parsed and parsed[\"summary\"]:\n",
        "             print(textwrap.fill(parsed[\"summary\"], width=100))\n",
        "        else:\n",
        "            print(\"Summary not available in the parsed output.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find the JSON object within the final output string.\")\n",
        "        print(\"Showing raw output:\")\n",
        "        print(final_output)\n",
        "\n",
        "except json.JSONDecodeError:\n",
        "    print(\"‚ö†Ô∏è Could not parse JSON. Showing raw output:\")\n",
        "    print(final_output)\n",
        "except KeyError as e:\n",
        "     print(f\"‚ö†Ô∏è KeyError: {e} - Check if expected keys are present in the JSON output.\")\n",
        "     print(\"Showing parsed dictionary (if available):\")\n",
        "     if 'parsed' in locals():\n",
        "         print(json.dumps(parsed, indent=2))\n",
        "     else:\n",
        "         print(\"Parsed dictionary not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8n3ytc6DdgR",
        "outputId": "1ea3446b-3f95-41df-a919-b7a09c29c863"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Final Metadata:\n",
            "{\n",
            "  \"title\": \"Credit Card Behaviour Score prediction Classification Risk base Techniques, Age Variable Variable bill statement correlation analysis using SMOTE Algorithm and ML model, Exploratory Data Analysis on Marriage Status and Age in a Credit Card Holder Dataset, Analyzing the Influence of Age on Customer Financial Behavior and Risk Profile, Exploratory Data Analysis for Credit Risk Prediction, SMOTE Algorithm Application in Credit Risk Portfolio, Tune Performance XGBoost Classifier for Credit Card Default Prediction, Machine Learning model predicts credit card defaulters, Machine Learning model predicts credit card default\",\n",
            "  \"author\": \"Mahanti Ajay Babu (Credit Card Behaviour Score prediction Classification Risk base Techniques), Not specified (others)\",\n",
            "  \"date\": \"\",\n",
            "  \"keywords\": [\n",
            "    \"classifier score\",\n",
            "    \"behaviour score\",\n",
            "    \"built credit\",\n",
            "    \"classification risk\",\n",
            "    \"credit cards\",\n",
            "    \"score prediction\",\n",
            "    \"card risk\",\n",
            "    \"models credit\",\n",
            "    \"introduction credit\",\n",
            "    \"manage credit\"\n",
            "  ],\n",
            "  \"document_type\": [\n",
            "    \"report\",\n",
            "    \"Research paper\",\n",
            "    \"Research Paper\",\n",
            "    \"Research Paper\",\n",
            "    \"Research Paper\",\n",
            "    \"Article\",\n",
            "    \"Article\",\n",
            "    \"Article\",\n",
            "    \"Article\",\n",
            "    \"Article\"\n",
            "  ],\n",
            "  \"summary\": \"This combined document discusses various approaches to credit risk analysis and prediction, focusing on the use of Machine Learning models, the SMOTE Algorithm, and Exploratory Data Analysis. Topics include credit card behavior score prediction, age variable correlation analysis, marriage status and age analysis, the influence of age on customer financial behavior, credit risk prediction, and the application of the SMOTE Algorithm in credit risk portfolio management. The documents emphasize the importance of data preprocessing, hyperparameter tuning, and model evaluation in developing accurate and reliable credit risk models.\"\n",
            "}\n",
            "\n",
            "‚úÖ Final Summary:\n",
            "This combined document discusses various approaches to credit risk analysis and prediction, focusing\n",
            "on the use of Machine Learning models, the SMOTE Algorithm, and Exploratory Data Analysis. Topics\n",
            "include credit card behavior score prediction, age variable correlation analysis, marriage status\n",
            "and age analysis, the influence of age on customer financial behavior, credit risk prediction, and\n",
            "the application of the SMOTE Algorithm in credit risk portfolio management. The documents emphasize\n",
            "the importance of data preprocessing, hyperparameter tuning, and model evaluation in developing\n",
            "accurate and reliable credit risk models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace Code_Jupyterfile_Rakesh.ipynb -q # for clear output"
      ],
      "metadata": {
        "id": "7jIU0TUwDlXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e12adb-fab6-4585-ac77-e400716a3bd9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: jupyter-nbconvert [-h] [--debug] [--show-config] [--show-config-json]\n",
            "                         [--generate-config] [-y] [--execute] [--allow-errors]\n",
            "                         [--stdin] [--stdout] [--inplace] [--clear-output]\n",
            "                         [--coalesce-streams] [--no-prompt] [--no-input]\n",
            "                         [--allow-chromium-download]\n",
            "                         [--disable-chromium-sandbox] [--show-input]\n",
            "                         [--embed-images] [--sanitize-html]\n",
            "                         [--log-level NbConvertApp.log_level]\n",
            "                         [--config NbConvertApp.config_file]\n",
            "                         [--to NbConvertApp.export_format]\n",
            "                         [--template TemplateExporter.template_name]\n",
            "                         [--template-file TemplateExporter.template_file]\n",
            "                         [--theme HTMLExporter.theme]\n",
            "                         [--sanitize_html HTMLExporter.sanitize_html]\n",
            "                         [--writer NbConvertApp.writer_class]\n",
            "                         [--post NbConvertApp.postprocessor_class]\n",
            "                         [--output NbConvertApp.output_base]\n",
            "                         [--output-dir FilesWriter.build_directory]\n",
            "                         [--reveal-prefix SlidesExporter.reveal_url_prefix]\n",
            "                         [--nbformat NotebookExporter.nbformat_version]\n",
            "                         [extra_args ...]\n",
            "jupyter-nbconvert: error: argument -q: expected one argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfBHo1uwMSkV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}